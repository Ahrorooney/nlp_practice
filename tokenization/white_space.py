#sample text
text = 'Tokenization splits text into words, phrases, symbols or other meaningful elements, which are called tokens.'

# White Space Tokenization using split()
tokens = text.split()

# Print tokens
print(tokens)